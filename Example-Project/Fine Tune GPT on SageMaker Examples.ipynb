{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tune GPT on SageMaker Examples\n",
    "Generative Pre-trained Transformer. In this example, we'll fine-tune a large GPT-2 on the Amazon SageMaker Examples code repository.\n",
    "\n",
    "First, let's process the raw notebook files and convert them into text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/awslabs/amazon-sagemaker-examples.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = '<your_bucket>'\n",
    "path = '<path_in_your_bucket>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# utils is a custom packaged I developed for this project\n",
    "from utils import parse_notebook, initialize_output\n",
    "\n",
    "def main(root_file, verbose, output_file, bucket, path):\n",
    "    '''\n",
    "    Takes a root file name, loops through all files.\n",
    "        When it finds an ipython notebook, pulls in for parsing.\n",
    "    '''\n",
    "    hits = 0\n",
    "    totals = 0\n",
    "    \n",
    "    for subdir, dirs, files in os.walk(root_file):\n",
    "\n",
    "        for file in files:\n",
    "\n",
    "            if '.ipynb' in file:\n",
    "                    \n",
    "                try:\n",
    "                    parse_notebook(input_file = os.path.join(subdir, file), output_file = output_file, bucket=bucket, path=path )\n",
    "                    if verbose:\n",
    "                        print ('worked for ', file)\n",
    "                    hits += 1\n",
    "                except:\n",
    "                    if verbose:\n",
    "                        print ('broke on ', file)\n",
    "                        \n",
    "                totals += 1\n",
    "                         \n",
    "    print ('Got {} hits out of {} total.'.format(hits, totals))\n",
    "    return\n",
    "\n",
    "output_file = \"sagemaker-examples.txt\"\n",
    "initialize_output(output_file)    \n",
    "verbose = False\n",
    "main('/root/amazon-sagemaker-examples', verbose , output_file, bucket, path )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now, let's copy that over to S3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_train_path = 's3://{}/{}/'.format(bucket, path)\n",
    "os.system('aws s3 cp {} {}'.format(output_file, s3_train_file) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's format a Python script that imports the large GPT-2 model, points to a fine-tuning framework, and applies our data on this model. \n",
    "\n",
    "Turns out we need a legacy version of TensorFlow to use this fine-tuning framework. In addition, when using script mode on this version of TensorFlow, we actually need to point to a bash script in the SageMaker training container to install our extra pacakges. Let's get that defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/bash_start.sh\n",
    "\n",
    "# install the extra packages\n",
    "pip install -r requirements.txt\n",
    "\n",
    "# run our script\n",
    "python tune_gpt.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/requirements.txt\n",
    "# tensorflow=1.14\n",
    "awscli\n",
    "gpt-2-simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, here's a modification of Max Woolf's nice gpt-2-simple package to fine tune GPT2 on a data file we bring ourselves. Thanks for the start code Max!\n",
    "- https://github.com/minimaxir/gpt-2-simple "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/tune_gpt.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/tune_gpt.py\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import gpt_2_simple as gpt2\n",
    "\n",
    "def get_model_name(model_size):\n",
    "    if 'large' in model_size:\n",
    "        model_name = '774M'\n",
    "\n",
    "    elif 'medium' in model_size: \n",
    "        model_name = \"355M\"\n",
    "\n",
    "    elif 'small' in model_size:\n",
    "        model_name = \"124M\"\n",
    "    \n",
    "    return model_name\n",
    "\n",
    "def get_file_name(bucket, path, file_name):\n",
    "    \n",
    "    # for the exceedingly time-constrained    \n",
    "    os.system('aws s3 cp s3://{}/{}/{} .'.format(bucket, path, file_name))\n",
    "    \n",
    "    return file_name\n",
    "\n",
    "def save_to_s3(txt, bucket, path, out_file):\n",
    "    \n",
    "    # say hello to cloudwatch\n",
    "    print (txt)\n",
    "\n",
    "    with open(outfile, 'w') as f:\n",
    "        f.write(txt)        \n",
    "\n",
    "    # could also save the trained model to s3 here\n",
    "#     os.environ.get('SM_MODEL_DIR')\n",
    "        \n",
    "    os.system('aws s3 cp {} s3://{}/{}/output/'.format(outfile, bucket, path))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "            \n",
    "    # turns out we need to hard code this in twice, once above, and another in the script here due to how the magic function %%writefile was implemented. \n",
    "    bucket = 'your-bucket'\n",
    "    \n",
    "    path = 'your-path'\n",
    "        \n",
    "    model_name = get_model_name('large')\n",
    "\n",
    "    if not os.path.isdir(os.path.join(\"models\", model_name)):\n",
    "        print(f\"Downloading {model_name} model...\")\n",
    "        gpt2.download_gpt2(model_name=model_name)   # model is saved into current directory under /models/model_name/\n",
    "\n",
    "    file_name = get_file_name(bucket, path, 'sagemaker-examples.txt')\n",
    "\n",
    "    sess = gpt2.start_tf_sess()\n",
    "\n",
    "    print ('fine tuning on {}'.format(file_name))\n",
    "    \n",
    "    gpt2.finetune(sess,\n",
    "                  file_name,\n",
    "                  model_name=model_name,\n",
    "                  steps=1000)   # steps is max number of training steps\n",
    "\n",
    "    txt = gpt2.generate(sess, return_as_list = True)[0]\n",
    "    \n",
    "    save_to_s3(txt, bucket, path, 'output.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've gotten that file written, just run your job!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "import os\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "region = sagemaker_session.boto_session.region_name\n",
    "\n",
    "est = TensorFlow(entry_point='bash_start.sh',\n",
    "                             role=role,\n",
    "                             source_dir = 'src',\n",
    "                             train_instance_count=1,\n",
    "                             \n",
    "                             # most accounts will need to explicitly request a limit increase for a GPU this large. just reach out to AWS support for this\n",
    "                             train_instance_type='ml.p3dn.24xlarge',\n",
    "                             framework_version='1.14',\n",
    "                             py_version='py3')\n",
    "\n",
    "# feel free to set wait to True here, or logs to True, if you want to see the results here. Otherwise, wait a few minutes, then open up cloudwatch to view your model training. \n",
    "est.fit(s3_train_path, wait=False)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/tensorflow-2.1-cpu-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
