{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import boto3\n",
    "import os\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "import sagemaker\n",
    "from random import shuffle\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "import csv\n",
    "import nltk\n",
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../Data/train.csv', names = list(range(89)))\n",
    "test = pd.read_csv('../Data/test.csv', names = list(range(89)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.array(train[0]).astype(\"float32\")\n",
    "train_features = np.array(train.drop(0, axis=1)).astype(\"float32\")\n",
    "test_labels = np.array(test[0]).astype(\"float32\")\n",
    "test_features  = np.array(test.drop(0, axis=1)).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_estimator(clf, sess, role):\n",
    "\n",
    "    container = get_image_uri(boto3.Session().region_name, clf)\n",
    "\n",
    "    est = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.m4.xlarge',\n",
    "                                    output_path='s3://{}/{}/output'.format(bucket, clf),\n",
    "                                    sagemaker_session=sess)\n",
    "    return est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimator(clf, sess, role):\n",
    "    \n",
    "    container = get_image_uri(boto3.Session().region_name, clf)\n",
    "\n",
    "    \n",
    "    if clf == 'xgboost':\n",
    "        est = get_base_estimator(clf, sess, role)\n",
    "        est.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        silent=0,\n",
    "                        objective='binary:logistic',\n",
    "                        num_round=100)\n",
    "        \n",
    "    elif clf == 'linear-learner':\n",
    "        \n",
    "        est = sagemaker.LinearLearner(role=sagemaker.get_execution_role(),\n",
    "                                               train_instance_count=1,\n",
    "                                               train_instance_type='ml.m4.xlarge',\n",
    "                                               predictor_type='binary_classifier',\n",
    "                                               num_classes=2)\n",
    "\n",
    "    elif clf == 'knn':\n",
    "        est = sagemaker.KNN(role=sagemaker.get_execution_role(),\n",
    "                                              k = 10,\n",
    "                                               train_instance_count=1,\n",
    "                                               train_instance_type='ml.m4.xlarge',\n",
    "                                               predictor_type='classifier',\n",
    "                                                sample_size = 200)\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "    elif clf == 'factorization-machines':\n",
    "        est = sagemaker.FactorizationMachines(role=sagemaker.get_execution_role(),\n",
    "                                               train_instance_count=1,\n",
    "                                               train_instance_type='ml.m4.xlarge',\n",
    "                                               predictor_type='binary_classifier',\n",
    "                                                num_factors = 2)\n",
    "        \n",
    "        \n",
    "    return est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add k-fold cross validation here \n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "client = boto3.client('sagemaker')\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "s3_input_train = sagemaker.s3_input(s3_data='s3://{}/train'.format(bucket), content_type='csv')\n",
    "s3_input_test = sagemaker.s3_input(s3_data='s3://{}/test/'.format(bucket), content_type='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.amazon.amazon_estimator import RecordSet\n",
    "import boto3\n",
    "\n",
    "# instantiate the LinearLearner estimator object\n",
    "multiclass_estimator = sagemaker.LinearLearner(role=sagemaker.get_execution_role(),\n",
    "                                               train_instance_count=1,\n",
    "                                               train_instance_type='ml.m4.xlarge',\n",
    "                                               predictor_type='binary_classifier',\n",
    "                                               num_classes=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap data in RecordSet objects\n",
    "train_records = multiclass_estimator.record_set(train_features, train_labels, channel='train')\n",
    "test_records = multiclass_estimator.record_set(test_features, test_labels, channel='test')\n",
    "\n",
    "# start a training job\n",
    "multiclass_estimator.fit([train_records, test_records])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tuner(clf, est):\n",
    "        \n",
    "    if clf == 'xgboost':\n",
    "        objective_metric_name = 'validation:auc'\n",
    "\n",
    "        hyperparameter_ranges = {'eta': ContinuousParameter(0, 1),\n",
    "                        'min_child_weight': ContinuousParameter(1, 10),\n",
    "                        'alpha': ContinuousParameter(0, 2),\n",
    "                        'max_depth': IntegerParameter(1, 10)}\n",
    "        \n",
    "    elif clf == 'knn':\n",
    "        \n",
    "        objective_metric_name = 'test:accuracy'\n",
    "\n",
    "        hyperparameter_ranges = {'k': IntegerParameter(1, 1024),\n",
    "                        'sample_size': IntegerParameter(256, 20000000)}\n",
    "        \n",
    "    elif clf == 'linear-learner':\n",
    "        objective_metric_name = 'test:recall'\n",
    "        \n",
    "        hyperparameter_ranges = {'l1': ContinuousParameter(0.0000001,1),\n",
    "                            'use_bias': CategoricalParameter([True, False])}\n",
    "        \n",
    "    elif clf == 'factorization-machines':\n",
    "        objective_metric_name = 'test:binary_classification_accuracy'\n",
    "        \n",
    "        hyperparameter_ranges = {'bias_wd': IntegerParameter(1, 1000)}\n",
    "        \n",
    "    tuner = HyperparameterTuner(est,\n",
    "                    objective_metric_name,\n",
    "                    hyperparameter_ranges,\n",
    "                    max_jobs=30,\n",
    "                    max_parallel_jobs=3)\n",
    "    \n",
    "    return tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_job(clf):\n",
    "\n",
    "    # build the estimator\n",
    "    est = get_estimator(clf, sess, role)\n",
    "\n",
    "    # get the hyperparameter tuner config \n",
    "    # set this to look for recall somehow \n",
    "    if clf == 'xgboost':\n",
    "        \n",
    "        tuner = get_tuner(clf, est)\n",
    "        \n",
    "        tuner.fit({'train': s3_input_train, 'test': s3_input_test}) \n",
    "\n",
    "    else:\n",
    "        # set the records\n",
    "        train_records = est.record_set(train_features, train_labels, channel='train')\n",
    "        test_records = est.record_set(test_features, test_labels, channel='validation')\n",
    "\n",
    "        tuner = get_tuner(clf, est)\n",
    "        \n",
    "        tuner.fit([train_records, test_records])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def magic_loop(models_to_run):\n",
    "    pool = Pool(processes=multiprocessing.cpu_count())\n",
    "    transformed_rows = pool.map(run_training_job, models_to_run)\n",
    "    pool.close() \n",
    "    pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = ['xgboost', 'linear-learner', 'factorization-machines', 'knn']\n",
    "magic_loop(clfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
