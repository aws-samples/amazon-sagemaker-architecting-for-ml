{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiprocess Ensembler\n",
    "Two things are important here. Your time, and your results. Let's see if we can optimize for both! Use this notebook when you already have train, test, and validation data. Then you can train & tune a large number of models, and pull the results back in using an ensembling approach that takes the maximum prediction out of each classifier.\n",
    "\n",
    "Finally, you'll use SageMaker Search to find the best performing models from your bucket, and run parallel batch transform jobs to run inference on all of your newly trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import boto3\n",
    "import os\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "import sagemaker\n",
    "from random import shuffle\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "import csv\n",
    "import nltk\n",
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the name of your bucket here\n",
    "bucket = 'ensemble-modeling'\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "client = boto3.client('sagemaker')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Upload your train and test data sets\n",
    "Make sure you have the label in the first column. If you don't already have a train and test set ready to go, try creating one with the amazon-sagemaker-example notebooks pre-installed on your notebook instance. That path is:\n",
    "- amazon-sagemaker-examples/introduction_to_applying_machine_learning/xgboost_direct_marketing\n",
    "\n",
    "If you run all cells in that notebook, you can copy the data to this directory with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /home/ec2-user/SageMaker/xgboost_direct_marketing_2019-08-12/train.csv .\n",
    "!cp /home/ec2-user/SageMaker/xgboost_direct_marketing_2019-08-12/validation.csv .\n",
    "!cp /home/ec2-user/SageMaker/xgboost_direct_marketing_2019-08-12/test.csv ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv', names = list(range(89)))\n",
    "validation = pd.read_csv('validation.csv', names = list(range(89)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.array(train[0]).astype(\"float32\")\n",
    "train_features = np.array(train.drop(0, axis=1)).astype(\"float32\")\n",
    "val_labels = np.array(validation[0]).astype(\"float32\")\n",
    "val_features  = np.array(validation.drop(0, axis=1)).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_estimator(clf, sess, role):\n",
    "\n",
    "    container = get_image_uri(boto3.Session().region_name, clf)\n",
    "\n",
    "    est = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.m4.xlarge',\n",
    "                                    output_path='s3://{}/{}/output'.format(bucket, clf),\n",
    "                                    sagemaker_session=sess)\n",
    "    return est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimator(clf, sess, role):\n",
    "    \n",
    "    container = get_image_uri(boto3.Session().region_name, clf)\n",
    "\n",
    "    \n",
    "    if clf == 'xgboost':\n",
    "        est = get_base_estimator(clf, sess, role)\n",
    "        est.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        silent=0,\n",
    "                        objective='binary:logistic',\n",
    "                        num_round=100)\n",
    "        \n",
    "    elif clf == 'linear-learner':\n",
    "        \n",
    "        est = sagemaker.LinearLearner(role=sagemaker.get_execution_role(),\n",
    "                                               train_instance_count=1,\n",
    "                                               train_instance_type='ml.m4.xlarge',\n",
    "                                               predictor_type='binary_classifier',\n",
    "                                               num_classes=2)\n",
    "\n",
    "    elif clf == 'knn':\n",
    "        est = sagemaker.KNN(role=sagemaker.get_execution_role(),\n",
    "                                              k = 10,\n",
    "                                               train_instance_count=1,\n",
    "                                               train_instance_type='ml.m4.xlarge',\n",
    "                                               predictor_type='classifier',\n",
    "                                                sample_size = 200)\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "    elif clf == 'factorization-machines':\n",
    "        est = sagemaker.FactorizationMachines(role=sagemaker.get_execution_role(),\n",
    "                                               train_instance_count=1,\n",
    "                                               train_instance_type='ml.m4.xlarge',\n",
    "                                               predictor_type='binary_classifier',\n",
    "                                                num_factors = 2)\n",
    "        \n",
    "        \n",
    "    return est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_to_s3(bucket):\n",
    "    os.system('aws s3 cp train.csv s3://{}/train/train.csv'.format(bucket))\n",
    "    os.system('aws s3 cp test.csv s3://{}/test/test.csv'.format(bucket))\n",
    "    os.system('aws s3 cp validation.csv s3://{}/validation/validation.csv'.format(bucket))\n",
    "        \n",
    "copy_to_s3(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tuner(clf, est):\n",
    "        \n",
    "    # this should search for the most recent hyperparameter tuning job, pull it in, and use for a warm start\n",
    "        \n",
    "    if clf == 'xgboost':\n",
    "        objective_metric_name = 'validation:auc'\n",
    "\n",
    "        hyperparameter_ranges = {'eta': ContinuousParameter(0, 1),\n",
    "                        'min_child_weight': ContinuousParameter(1, 10),\n",
    "                        'alpha': ContinuousParameter(0, 2),\n",
    "                        'max_depth': IntegerParameter(1, 10)}\n",
    "        \n",
    "    elif clf == 'knn':\n",
    "        \n",
    "        objective_metric_name = 'test:accuracy'\n",
    "\n",
    "        hyperparameter_ranges = {'k': IntegerParameter(1, 1024),\n",
    "                        'sample_size': IntegerParameter(256, 20000000)}\n",
    "        \n",
    "    elif clf == 'linear-learner':\n",
    "        objective_metric_name = 'test:recall'\n",
    "        \n",
    "        hyperparameter_ranges = {'l1': ContinuousParameter(0.0000001,1),\n",
    "                            'use_bias': CategoricalParameter([True, False])}\n",
    "        \n",
    "    elif clf == 'factorization-machines':\n",
    "        objective_metric_name = 'test:binary_classification_accuracy'\n",
    "        \n",
    "        hyperparameter_ranges = {'bias_wd': IntegerParameter(1, 1000)}\n",
    "        \n",
    "    tuner = HyperparameterTuner(est,\n",
    "                    objective_metric_name,\n",
    "                    hyperparameter_ranges,\n",
    "                    max_jobs=30,\n",
    "                    max_parallel_jobs=3)\n",
    "    \n",
    "    return tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_job(clf):\n",
    "\n",
    "    \n",
    "    # this should loop through splits in k-fold cross validation\n",
    "    \n",
    "    # build the estimator\n",
    "    est = get_estimator(clf, sess, role)\n",
    "\n",
    "    # get the hyperparameter tuner config \n",
    "    if clf == 'xgboost':\n",
    "        \n",
    "        tuner = get_tuner(clf, est)\n",
    "        \n",
    "        \n",
    "        tuner.fit({'train': s3_input_train, 'validation': s3_input_validation}) \n",
    "\n",
    "    else:\n",
    "        # set the records\n",
    "        train_records = est.record_set(train_features, train_labels, channel='train')\n",
    "        test_records = est.record_set(val_features, val_labels, channel='validation')\n",
    "\n",
    "        tuner = get_tuner(clf, est)\n",
    "        \n",
    "        tuner.fit([train_records, test_records])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def magic_loop(models_to_run):\n",
    "    pool = Pool(processes=multiprocessing.cpu_count())\n",
    "    transformed_rows = pool.map(run_training_job, models_to_run)\n",
    "    pool.close() \n",
    "    pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_train = sagemaker.s3_input(s3_data='s3://{}/train'.format(bucket), content_type='csv')\n",
    "\n",
    "s3_input_validation = sagemaker.s3_input(s3_data='s3://{}/validation/'.format(bucket), content_type='csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define the models you want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clfs = ['xgboost', 'linear-learner', 'factorization-machines', 'knn']\n",
    "\n",
    "clfs = [ 'xgboost']\n",
    "\n",
    "magic_loop(clfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Select the best models\n",
    "Now, we're going to use SageMaker search to find the best performing models from the hyperparameter tuning jobs we just ran."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "smclient = boto3.client(service_name='sagemaker')\n",
    "\n",
    "# Search the training job by Amazon S3 location of model artifacts\n",
    "search_params={\n",
    "   \"MaxResults\": 100,\n",
    "   \"Resource\": \"TrainingJob\",\n",
    "   \"SearchExpression\": { \n",
    "      \"Filters\": [ \n",
    "         { \n",
    "            \"Name\": \"InputDataConfig.DataSource.S3DataSource.S3Uri\",\n",
    "            \"Operator\": \"Contains\",\n",
    "             \n",
    "             # set this to have a word that is in your bucket name\n",
    "            \"Value\": '{}'.format(bucket)\n",
    "         },\n",
    "        { \n",
    "            \"Name\": \"TrainingJobStatus\",\n",
    "            \"Operator\": \"Equals\",\n",
    "            \"Value\": 'Completed'\n",
    "         }, \n",
    "    ],\n",
    "     \n",
    "   },\n",
    "    \n",
    "    \"SortBy\": \"Metrics.validation:auc\",\n",
    "    \"SortOrder\": \"Descending\"\n",
    "}\n",
    "results = smclient.search(**search_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "\n",
    "def get_models(results):\n",
    "\n",
    "    role = sagemaker.get_execution_role()\n",
    "\n",
    "    models = []\n",
    "\n",
    "    for each in results['Results']:\n",
    "\n",
    "        job_name = each['TrainingJob']['TrainingJobName']\n",
    "\n",
    "\n",
    "        artifact = each['TrainingJob']['ModelArtifacts']['S3ModelArtifacts']\n",
    "\n",
    "        # get training image\n",
    "        image =  each['TrainingJob']['AlgorithmSpecification']['TrainingImage']\n",
    "\n",
    "        m = Model(artifact, image, role = role, sagemaker_session = sess, name = job_name)\n",
    "\n",
    "        models.append(m)\n",
    "        \n",
    "    return models[:15]\n",
    "\n",
    "models = get_models(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Ensemble Batch Transform\n",
    "Now, we're going to run a separate batch transform job for each model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using already existing model: ee-190801-1852-030-9332e42c\n",
      "Using already existing model: ee-190801-1852-027-c1697b33\n",
      "Using already existing model: xgboost-190801-1850-024-bb419b7b\n",
      "Using already existing model: ee-190801-1852-007-e64ab3dc\n",
      "Using already existing model: ee-190801-1852-017-d9b8d13d\n",
      "Using already existing model: xgboost-190801-1850-016-53897fd8\n",
      "Using already existing model: xgboost-190801-1850-004-6a0fda05\n",
      "Using already existing model: ee-190801-1852-016-5401644a\n",
      "Using already existing model: xgboost-190801-1850-006-2f5b942e\n",
      "Using already existing model: ee-190801-1852-028-19d1cc27\n",
      "Using already existing model: xgboost-190801-1850-029-5cfc670c\n",
      "Using already existing model: ee-190801-1852-018-43d4a224\n",
      "Using already existing model: ee-190801-1852-011-97ee218e\n",
      "Using already existing model: xgboost-190801-1850-005-66a65cac\n",
      "Using already existing model: ee-190801-1852-010-03ce44d3\n"
     ]
    }
   ],
   "source": [
    "def run_batch_transform(model, bucket):\n",
    "\n",
    "    transformer = model.transformer(\n",
    "        instance_count=1,\n",
    "        instance_type='ml.m4.xlarge',\n",
    "        output_path='s3://{}/batch_results/{}'.format(bucket, model.name)\n",
    "    )\n",
    "\n",
    "    transformer.transform(data='s3://{}/test/test.csv'.format(bucket), content_type='text/csv')\n",
    "\n",
    "    \n",
    "for model in models:\n",
    "    run_batch_transform(model, bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Consolidate batch results\n",
    "Finally, we'll pull together all of the batch job inferences. For each one, we'll take the maximum confidence level and consider that a positive prediction. Then we'll see how well that performs, relative to using a single XGBoost model. \n",
    "\n",
    "This next cell is going to copy everything in the S3 bucket path under batch results to your local notebook instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('aws s3 sync s3://{}/batch_results/ /home/ec2-user/SageMaker/batch_results/'.format(bucket))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe():\n",
    "    '''\n",
    "    Loops through the directory on your local notebook instance where the batch results were stored, \n",
    "        and generates a dataframe where each column is the output from a different model.\n",
    "    '''\n",
    "    frames  = []\n",
    "    \n",
    "    for sub_dir in os.listdir('/home/ec2-user/SageMaker/batch_results'):\n",
    "        if '.ipynb' not in sub_dir and '.out' not in sub_dir:\n",
    "\n",
    "            old_file = '/home/ec2-user/SageMaker/batch_results/{}/test.csv.out'.format(sub_dir)\n",
    "            \n",
    "            new_file = '/home/ec2-user/SageMaker/batch_results/{}/test.csv'.format(sub_dir)\n",
    "            \n",
    "            # remove the .out file formate\n",
    "            os.system('cp {} {}'.format( old_file, new_file))\n",
    "            \n",
    "            df = pd.read_csv('/home/ec2-user/SageMaker/batch_results/{}/test.csv'.format(sub_dir), names = [sub_dir])\n",
    "\n",
    "            frames.append(df)\n",
    "            \n",
    "    df = pd.concat(frames, axis=1)\n",
    "                \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate_results(df):\n",
    "\n",
    "    df['max'] = 0\n",
    "    df['min'] = 0\n",
    "    df['diff'] = 0\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "\n",
    "        top = max(row)\n",
    "        bottom = min(row)\n",
    "\n",
    "        diff = top - bottom\n",
    "\n",
    "        df.loc[idx, 'max'] = top\n",
    "        df.loc[idx, 'min'] = bottom\n",
    "        df.loc[idx, 'diff'] = diff\n",
    "\n",
    "    return df\n",
    "\n",
    "bare_df = get_dataframe()\n",
    "consolidated_df = consolidate_results(bare_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_label_to_results(df):\n",
    "    test_data = pd.read_csv('test.csv')\n",
    "    y_true = test_data['0'].values.tolist()\n",
    "    df['y_true'] = y_true\n",
    "    return df\n",
    "    \n",
    "    \n",
    "df = add_label_to_results(consolidated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>xgboost-190730-1958-013-578ad74a</th>\n",
       "      <th>xgboost-190730-2044-005-b614948b</th>\n",
       "      <th>xgboost-190730-2020-026-b0f76816</th>\n",
       "      <th>xgboost-190730-1958-023-2e4c37ce</th>\n",
       "      <th>xgboost-190730-2020-029-b7c0ecf1</th>\n",
       "      <th>xgboost-190730-2020-028-3a863835</th>\n",
       "      <th>xgboost-190730-2044-004-739de6d8</th>\n",
       "      <th>xgboost-190730-2044-022-2352eede</th>\n",
       "      <th>xgboost-190730-2044-021-949ec8ac</th>\n",
       "      <th>xgboost-190730-1958-015-b671b4dc</th>\n",
       "      <th>xgboost-190730-2020-019-c5dba7b3</th>\n",
       "      <th>xgboost-190730-1958-024-b8a2fd71</th>\n",
       "      <th>xgboost-190730-2044-019-440eba24</th>\n",
       "      <th>xgboost-190730-1958-021-72e38862</th>\n",
       "      <th>xgboost-190730-2020-030-483f572c</th>\n",
       "      <th>max</th>\n",
       "      <th>min</th>\n",
       "      <th>diff</th>\n",
       "      <th>y_true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.089065</td>\n",
       "      <td>0.092084</td>\n",
       "      <td>0.108009</td>\n",
       "      <td>0.094674</td>\n",
       "      <td>0.098098</td>\n",
       "      <td>0.097160</td>\n",
       "      <td>0.103718</td>\n",
       "      <td>0.100188</td>\n",
       "      <td>0.098029</td>\n",
       "      <td>0.086106</td>\n",
       "      <td>0.090717</td>\n",
       "      <td>0.074131</td>\n",
       "      <td>0.092530</td>\n",
       "      <td>0.098769</td>\n",
       "      <td>0.104903</td>\n",
       "      <td>0.108009</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.108009</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.189402</td>\n",
       "      <td>0.179116</td>\n",
       "      <td>0.210121</td>\n",
       "      <td>0.141365</td>\n",
       "      <td>0.208647</td>\n",
       "      <td>0.168847</td>\n",
       "      <td>0.176325</td>\n",
       "      <td>0.168533</td>\n",
       "      <td>0.171908</td>\n",
       "      <td>0.179298</td>\n",
       "      <td>0.190306</td>\n",
       "      <td>0.186766</td>\n",
       "      <td>0.182808</td>\n",
       "      <td>0.195914</td>\n",
       "      <td>0.189338</td>\n",
       "      <td>0.210121</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.210121</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.242742</td>\n",
       "      <td>0.254854</td>\n",
       "      <td>0.273822</td>\n",
       "      <td>0.267924</td>\n",
       "      <td>0.250157</td>\n",
       "      <td>0.256963</td>\n",
       "      <td>0.293109</td>\n",
       "      <td>0.282113</td>\n",
       "      <td>0.277161</td>\n",
       "      <td>0.222159</td>\n",
       "      <td>0.270040</td>\n",
       "      <td>0.251583</td>\n",
       "      <td>0.278672</td>\n",
       "      <td>0.275336</td>\n",
       "      <td>0.251772</td>\n",
       "      <td>0.293109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.293109</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.029594</td>\n",
       "      <td>0.035632</td>\n",
       "      <td>0.021417</td>\n",
       "      <td>0.031481</td>\n",
       "      <td>0.026864</td>\n",
       "      <td>0.030731</td>\n",
       "      <td>0.039072</td>\n",
       "      <td>0.040475</td>\n",
       "      <td>0.040877</td>\n",
       "      <td>0.030913</td>\n",
       "      <td>0.036439</td>\n",
       "      <td>0.024260</td>\n",
       "      <td>0.038267</td>\n",
       "      <td>0.028601</td>\n",
       "      <td>0.035436</td>\n",
       "      <td>0.040877</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040877</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.031521</td>\n",
       "      <td>0.032894</td>\n",
       "      <td>0.031920</td>\n",
       "      <td>0.040215</td>\n",
       "      <td>0.029252</td>\n",
       "      <td>0.033342</td>\n",
       "      <td>0.038584</td>\n",
       "      <td>0.039658</td>\n",
       "      <td>0.040352</td>\n",
       "      <td>0.031937</td>\n",
       "      <td>0.035477</td>\n",
       "      <td>0.037843</td>\n",
       "      <td>0.037440</td>\n",
       "      <td>0.028855</td>\n",
       "      <td>0.031971</td>\n",
       "      <td>0.040352</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040352</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   xgboost-190730-1958-013-578ad74a  xgboost-190730-2044-005-b614948b  \\\n",
       "0                          0.089065                          0.092084   \n",
       "1                          0.189402                          0.179116   \n",
       "2                          0.242742                          0.254854   \n",
       "3                          0.029594                          0.035632   \n",
       "4                          0.031521                          0.032894   \n",
       "\n",
       "   xgboost-190730-2020-026-b0f76816  xgboost-190730-1958-023-2e4c37ce  \\\n",
       "0                          0.108009                          0.094674   \n",
       "1                          0.210121                          0.141365   \n",
       "2                          0.273822                          0.267924   \n",
       "3                          0.021417                          0.031481   \n",
       "4                          0.031920                          0.040215   \n",
       "\n",
       "   xgboost-190730-2020-029-b7c0ecf1  xgboost-190730-2020-028-3a863835  \\\n",
       "0                          0.098098                          0.097160   \n",
       "1                          0.208647                          0.168847   \n",
       "2                          0.250157                          0.256963   \n",
       "3                          0.026864                          0.030731   \n",
       "4                          0.029252                          0.033342   \n",
       "\n",
       "   xgboost-190730-2044-004-739de6d8  xgboost-190730-2044-022-2352eede  \\\n",
       "0                          0.103718                          0.100188   \n",
       "1                          0.176325                          0.168533   \n",
       "2                          0.293109                          0.282113   \n",
       "3                          0.039072                          0.040475   \n",
       "4                          0.038584                          0.039658   \n",
       "\n",
       "   xgboost-190730-2044-021-949ec8ac  xgboost-190730-1958-015-b671b4dc  \\\n",
       "0                          0.098029                          0.086106   \n",
       "1                          0.171908                          0.179298   \n",
       "2                          0.277161                          0.222159   \n",
       "3                          0.040877                          0.030913   \n",
       "4                          0.040352                          0.031937   \n",
       "\n",
       "   xgboost-190730-2020-019-c5dba7b3  xgboost-190730-1958-024-b8a2fd71  \\\n",
       "0                          0.090717                          0.074131   \n",
       "1                          0.190306                          0.186766   \n",
       "2                          0.270040                          0.251583   \n",
       "3                          0.036439                          0.024260   \n",
       "4                          0.035477                          0.037843   \n",
       "\n",
       "   xgboost-190730-2044-019-440eba24  xgboost-190730-1958-021-72e38862  \\\n",
       "0                          0.092530                          0.098769   \n",
       "1                          0.182808                          0.195914   \n",
       "2                          0.278672                          0.275336   \n",
       "3                          0.038267                          0.028601   \n",
       "4                          0.037440                          0.028855   \n",
       "\n",
       "   xgboost-190730-2020-030-483f572c       max  min      diff  y_true  \n",
       "0                          0.104903  0.108009  0.0  0.108009       1  \n",
       "1                          0.189338  0.210121  0.0  0.210121       1  \n",
       "2                          0.251772  0.293109  0.0  0.293109       1  \n",
       "3                          0.035436  0.040877  0.0  0.040877       0  \n",
       "4                          0.031971  0.040352  0.0  0.040352       0  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Generate Confusion Matrix\n",
    "At the end, let's chart a plot for the performance of each of these models. Did the ensembling help? Which model appears to be the best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confusion_matrix(df, model_column, accuracy=None):\n",
    "    \n",
    "    mx = pd.crosstab(index=df['y_true'], columns=np.round(df[model_column]), rownames=['actuals'], colnames=['predictions'])\n",
    "\n",
    "    # lower right corner\n",
    "    tps = mx.iloc[1, 1]\n",
    "        \n",
    "    # upper right corner\n",
    "    fps = mx.iloc[0, 1]\n",
    "    \n",
    "    # lower left corner\n",
    "    fns = mx.iloc[1, 0]\n",
    "    \n",
    "    precision = np.round(tps / (tps + fns), 4) * 100\n",
    "    \n",
    "    recall = np.round(tps / (tps + fps), 4) * 100\n",
    "    \n",
    "    print ('Precision = {}%, Recall = {}%'.format(precision, recall))\n",
    "    \n",
    "    if accuracy:\n",
    "        \n",
    "        # upper left corner \n",
    "        tns = mx.iloc[0, 0]\n",
    "        \n",
    "        accuracy = (tps + tns) / (fns + fps + tps + tns) * 100\n",
    "        \n",
    "        print ('Overall binary classification accuracy = {}%'.format(accuracy))\n",
    "        \n",
    "    return mx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Without Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 19.25%, Recall = 67.88%\n",
      "Overall binary classification accuracy = 89.46090335114133%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>predictions</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actuals</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3591</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>390</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "predictions   0.0  1.0\n",
       "actuals               \n",
       "0            3591   44\n",
       "1             390   93"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_confusion_matrix(df,'xgboost-190730-2044-004-739de6d8', accuracy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results With Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 26.090000000000003%, Recall = 61.760000000000005%\n",
      "Overall binary classification accuracy = 89.43661971830986%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>predictions</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actuals</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3557</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>357</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "predictions   0.0  1.0\n",
       "actuals               \n",
       "0            3557   78\n",
       "1             357  126"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_confusion_matrix(df, 'max', accuracy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting. Using this method we saw precision increase by 7 percentage points, but recall dropped by 7 points. Overall classfication accuracy did not seem to increase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
